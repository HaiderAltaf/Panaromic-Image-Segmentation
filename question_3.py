# -*- coding: utf-8 -*-
"""Question_3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L3mPAraoEmdFixF1TJTXy0MYLQITaHy8

#Implement the backpropagation algorithm with support for the following optimisation functions

    

*  sgd

*  momentum based gradient descent
*  nesterov accelerated gradient descent
*  rmsprop
*  adam
*  nadam
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

from activation_fun import sigmoid, tanh, ReLu, softmax, der_sigmoid, der_tanh, der_ReLu, der_softmax
from loss_accuracy import model_loss, model_accuracy
from question_2 import forward_propagation
from weights_bias import weights_bias

def batch_normalize(a):
    mean = np.mean(a, axis=0, keepdims=True)
    var = np.var(a, axis=0, keepdims=True)
    a_norm = (a - mean) / np.sqrt(var + 1e-5)
    return a_norm

def backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                         hid_layer, loss_fu, acti_fun, L2_decay):
  
  L = hid_layer
  grad_W = [0]*L
  grad_b = [0]*L

  
  ## change each y_train into an array of 10 values
  y_train_modified = np.zeros(10)
  y_train_modified[y_train] = 1
  
  L2_loss = 0
  
  for i in range(len(Weights)):
      L2_loss += L2_decay*np.sum(Weights[i])/len(X_train)
  if loss_fu =='cross_entropy':
    output_gradient = -(y_train_modified - y_dash) + L2_loss
  elif loss_fu == 'mse':
    output_gradient = (y_dash-y_train_modified )*der_softmax(a_out[L-1]) + L2_loss
    
  for k in range(L, 0, -1):

    ## compute gradients w.r.t parameters
    W_gradient = np.matmul(output_gradient.reshape(len(output_gradient),1), 
                           h_out[k-1].reshape(1,len(h_out[k-1]))) 
    grad_W[k-1] = W_gradient

    b_gradients = output_gradient 
    grad_b[k-1] = b_gradients
   
    if k==1:
      continue
    ## compute gradients w.r.t layer below
    weight = Weights[k-1]
    h_gradient = np.matmul(weight.T, output_gradient)

    ## compute the gradient of pre activation layer
    if acti_fun == 'sigmoid':
      output_gradient = np.multiply(h_gradient, der_sigmoid(a_out[k-2]))
    elif acti_fun == 'ReLu':
      output_gradient = np.multiply(h_gradient, der_ReLu(a_out[k-2]))
    elif acti_fun == 'tanh':
     output_gradient = np.multiply(h_gradient, der_tanh(a_out[k-2]))
    

  return grad_W, grad_b

"""#Mini Batch Gradient Descent 
if batch_size = 1, the algorithm will be stochastic gradient descent.
if batch_size = Number of samples, the algorithm will be vanilla gradient descent

"""

def gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,
                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun,
                     weight_init):
  
  no_of_classes = len(np.unique(y_train))
  L = hid_layer
  n = no_of_neuron
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)

  ## initialize the count of images paased
  loss =0
  num_points_seen = 0

  for x,y in zip(X_train,y_train):


    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)

    ## Adding the gradients of weights and biases
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]
    
    num_points_seen+=1
    
    if num_points_seen%batch_size == 0:
        
      #if acti_fun == 'ReLu':
      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]

      # Weights updates
      
      Weights = [Weights[i] - dW[i]*learning_rate for i in range(L)]
    
#       # normalize the weights if activation function is ReLu
#       if acti_fun == 'ReLu':
#          Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - dB[i]*learning_rate for i in range(L)]

      
      ## initialize the gradients of weights and biases
      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)

  # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
               L2_decay, X_train, acti_fun,weight_init)
 

  #print(epoch, loss)

  return Weights, bias, loss

"""#Momentum Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,
                no_of_neuron, X_train, y_train,learning_rate,
                batch_size, L2_decay, loss_fu, acti_fun, weight_init):
  
  beta = 0.9
  no_of_classes = len(np.unique(y_train))
  L = hid_layer
  n = no_of_neuron
  
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
  
  # initialize the sample count
  num_points_seen = 0

  for x,y in zip(X_train, y_train):

    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)

    ## Adding the gradients of weights and biases
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:
      
     # normalizing the gradient
      dW = [batch_normalize(dW[i]) for i in range(len(dW))]

      ## momentum based wight updates
      uw = [prev_uw[i]*beta + dW[i] for i in range(len(dW))]
      ub = [prev_ub[i]*beta + dB[i] for i in range(len(dB))]
      
      ## Weights and biases updates
      # Weights updates
      Weights = [Weights[i] - uw[i]*learning_rate for i in range(len(uw))]
    
#       # normalize the weights if activation function is ReLu
#       if acti_fun == 'ReLu':
      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - ub[i]*learning_rate for i in range(len(ub))]

      # assign present to the history 
      prev_uw = uw
      prev_ub = ub

      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
  
   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)
  

  return  Weights, bias, loss

"""#NAG Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def nag(prev_vw, prev_vb, Weights, bias, hid_layer,
        no_of_neuron, X_train, y_train,learning_rate,
        batch_size, L2_decay, loss_fu, acti_fun, weight_init):
    
  
  beta = 0.9
  no_of_classes = len(np.unique(y_train))
  L = hid_layer
  n = no_of_neuron
  
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
  
  num_points_seen = 0

  # do partial updates
  v_w = [beta*prev_vw[i] for i in range(len(prev_vw))]
  v_b = [beta*prev_vb[i] for i in range(len(prev_vb))]

  for x, y in zip(X_train, y_train):


    ## Forward propagation
   
    Weights = [Weights[i]-v_w[i] for i in range(len(Weights))]
    bias    = [bias[i]-v_b[i] for i in range(len(bias))]
    
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)
    

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights, 
                                          hid_layer, loss_fu, acti_fun, L2_decay)

    ## Look Ahead
    ## Adding the gradients of weights and biases
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]
    
    num_points_seen +=1

    if num_points_seen%batch_size==0:
        
       # normalize the gradient if activation function is ReLu
      #if acti_fun == 'ReLu':
      #dW = [batch_normalize(dW[i]) for i in range(len(dW))]


      ## momentum based wight updates
      vw = [prev_vw[i]*beta + dW[i] for i in range(len(dW))]
      vb = [prev_vb[i]*beta + dB[i] for i in range(len(dB))]

      ## Weights and biases updates
      Weights = [Weights[i] - vw[i]*learning_rate for i in range(len(vw))]
        
#       # normalize the weights if activation function is ReLu
#       if acti_fun == 'ReLu':
      #Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - vb[i]*learning_rate for i in range(len(vb))]

      # assign present to the history 
      prev_uw = vw
      prev_ub = vb

      #dW,dB = weights_bias(3, n, L, X_train, no_of_classes)
      
   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)


  return  Weights, bias, loss

"""#AdaGrad Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def adagrad(v_w, v_b, Weights, bias, hid_layer, no_of_neuron, X_train, y_train,
            learning_rate, batch_size, L2_decay, loss_fu, acti_fun,weight_init):
  
  v_w, v_b = weights_bias(3, no_of_neuron, hid_layer, X_train, 10)
  
  eps = 1e-10
  no_of_classes = len(np.unique(y_train))
  L = hid_layer
  n = no_of_neuron
  
  ## initialize the gradients of weights and biases
  dW,dB = weights_bias(3, n, L, X_train, no_of_classes)
   
  ## initialize the count 
  num_points_seen = 0

  for x, y in zip(X_train, y_train):

    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)

    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:

      ## Add L2 regularization penalty to gradient
      dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]

      #compute intermediate values
      v_w = [v_w[i] + dW[i]**2 for i in range(len(grad_W))]
      v_b = [v_b[i] + dB[i]**2 for i in range(len(grad_b))]

      # Weights updates
      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]
    
      # normalize the weights if activation function is ReLu
      #if acti_fun == 'ReLu':
      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]

      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
      #dB = biases(3, n, L, y_train, no_of_classes)
  
   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)


  return  Weights, bias, loss

"""#RMSProp Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,
            X_train, y_train, learning_rate, batch_size,
            L2_decay,loss_fu, acti_fun, weight_init):
  
  eps = 1e-10
  beta = 0.9
  no_of_classes = len(np.unique(y_train))
  L = hid_layer
  n = no_of_neuron
  
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
  
  ## initialize the sample count
  num_points_seen = 0

  for x, y in zip(X_train, y_train):


    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)
    
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:
        
      dW = [batch_normalize(dW[i]) for i in range(len(dW))]

      ## Add L2 regularization penalty to gradient
      #dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]

      #compute intermediate values
      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]
      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]

      ## Weights and biases updates
      # Weights updates
      Weights = [Weights[i] - learning_rate*dW[i]/(np.sqrt(v_w[i])+eps) for i in range(len(Weights))]
        
      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]
 
      # Biases updates
      bias = [bias[i] - learning_rate*dB[i]/(np.sqrt(v_b[i])+eps) for i in range(len(bias))]

      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)
  
   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)

  return  Weights, bias, loss

"""#AdaDelta Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def adaDelta(u_w, u_b, v_w, v_b, Weights, bias, hid_layer,
             no_of_neuron, X_train, y_train, batch_size,
              L2_decay, loss_fu, acti_fun, weight_init):


  beta = 0.9
  eps = 1e-10
  no_of_classes = len(np.unique(y_train))
  L, n = hid_layer, no_of_neuron



  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, n, L, X_train, no_of_classes)

  ## initialize the sample count
  num_points_seen = 0

  for x, y in zip(X_train, y_train):

    #x = np.float128(x)

    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)

    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:
        
      # normalize the gradient if activation function is ReLu
      #if acti_fun == 'ReLu':
      dW = [batch_normalize(dW[i]) for i in range(len(dW))]

      ## Add L2 regularization penalty to gradient
     # dW = [dW[i] + L2_decay*Weights[i] for i in range(len(dW))]

      #compute intermediate values
      v_w = [beta*v_w[i] + (1-beta)*(dW[i]**2) for i in range(len(grad_W))]
      v_b = [beta*v_b[i] + (1-beta)*(dB[i]**2) for i in range(len(grad_b))]

      del_w = [dW[i]*np.sqrt(u_w[i]+eps)/(np.sqrt(v_w[i]+eps)) for i in range(len(dW))]
      del_b = [dB[i]*np.sqrt(u_b[i]+eps)/(np.sqrt(v_b[i]+eps)) for i in range(len(dB))]

      u_w = [beta*u_w[i] + (1-beta)*del_w[i]**2 for i in range(len(u_w))]
      u_b = [beta*u_b[i] + (1-beta)*del_b[i]**2 for i in range(len(u_b))]

      # Weights updates
      Weights = [Weights[i] - del_w[i] for i in range(len(del_w))]
        
#       # normalize the weights if activation function is ReLu
#       if acti_fun == 'ReLu':
      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - del_b[i] for i in range(len(del_b))]

      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)

   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)


  return  Weights, bias, loss

"""#Adam Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def adam(epoch, m_w, m_b, v_w, v_b, Weights,
         bias, hid_layer, no_of_neuron, X_train, y_train, 
         batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init):
  
  eps = 1e-10
  beta1 = 0.9
  beta2 = 0.999  
  no_of_classes = len(np.unique(y_train))
  L, n = hid_layer, no_of_neuron
    
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)
  
  ## initialize the sample count
  num_points_seen = 0

  for x, y in zip(X_train, y_train):

    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)
    
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:
      
     # normalize the gradient if activation function is ReLu
#       if acti_fun == 'ReLu':
      dW = [batch_normalize(dW[i]) for i in range(len(dW))]
            

      #compute intermediate values
      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]
      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]

      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]
      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]

      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]
      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]

      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]
      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]

      # Weights updates
      Weights = [Weights[i] - learning_rate*m_w_hat[i]/(np.sqrt(v_w_hat[i])+eps) for i in range(len(Weights))]
      
#       # normalize the weights if activation function is ReLu
#       if acti_fun == 'ReLu':
      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - learning_rate*m_b_hat[i]/(np.sqrt(v_b_hat[i])+eps) for i in range(len(Weights))]

      dW, dB = weights_bias(3, n, L, X_train, no_of_classes)

   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)


  return Weights, bias, loss

"""#NAdam Gradient Descent
if batch_size = 1, the algorithm will be stochastic gradient descent. if batch_size = Number of samples, the algorithm will be vanilla gradient descent
"""

def nadam(epoch, m_w, m_b, v_w, v_b, Weights, bias,
          hid_layer, no_of_neuron, X_train, y_train, batch_size,
          learning_rate,L2_decay, loss_fu, acti_fun, weight_init):
  
  eps = 1e-10
  beta1 = 0.9
  beta2 = 0.999
  no_of_classes = len(np.unique(y_train))
  L, n = hid_layer, no_of_neuron
  ## initialize the gradients of weights and biases
  dW, dB = weights_bias(3,no_of_neuron, hid_layer, X_train, no_of_classes)

  ## initialize the sample count
  num_points_seen = 0

  for x, y in zip(X_train, y_train):

    #x = np.float128(x)

    ## Forward propagation
    a_out, h_out, y_dash = forward_propagation(Weights, bias, x, L, acti_fun, weight_init)

    ## Backward Propagation
    grad_W, grad_b = backward_propagation(X_train, batch_size, a_out, h_out, y_train, y_dash, Weights,
                                          hid_layer, loss_fu, acti_fun, L2_decay)
    
    dW = [dW[i] + grad_W[i] for i in range(len(dW))]
    dB = [dB[i] + grad_b[i] for i in range(len(dB))]

    num_points_seen +=1

    if num_points_seen%batch_size==0:
        
      # normalize the gradient if activation function is ReLu
      #if acti_fun == 'ReLu':
      dW = [batch_normalize(dW[i]) for i in range(len(dW))]

      ## Add L2 regularization penalty to gradient
      #dW = [dW[i] + L2_decay*Weights[i]/batch_size for i in range(len(dW))]

      #compute intermediate values
      m_w = [beta1*m_w[i] + (1-beta1)*dW[i] for i in range(len(m_w))]
      m_b = [beta1*m_b[i] + (1-beta1)*dB[i] for i in range(len(m_b))]

      v_w = [beta2*v_w[i] + (1-beta2)*(dW[i]**2) for i in range(len(dW))]
      v_b = [beta2*v_b[i] + (1-beta2)*(dB[i]**2) for i in range(len(dB))]

      m_w_hat = [m_w[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_w))]
      m_b_hat = [m_b[i]/(1-np.power(beta1,epoch+1)) for i in range(len(m_b))]

      v_w_hat = [v_w[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_w))]
      v_b_hat = [v_b[i]/(1-np.power(beta2,epoch+1)) for i in range(len(v_b))]

      # Weights updates
      Weights = [Weights[i] - (learning_rate/np.sqrt(v_w_hat[i]+eps))*(beta1*m_w_hat[i]+(1-beta1)*dW[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]
      
      Weights = [batch_normalize(Weights[i]) for i in range(len(Weights))]

      # Biases updates
      bias = [bias[i] - (learning_rate/np.sqrt(v_b_hat[i]+eps))*(beta1*m_b_hat[i]+(1-beta1)*dB[i]/(1-beta1**(epoch+1))) for i in range(len(Weights))]

      dW, dB = weights_bias(3, no_of_neuron, hid_layer, X_train, no_of_classes)

   # Training loss of an epoch
  loss  = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu,
                     L2_decay, X_train, acti_fun, weight_init)

  return Weights, bias, loss