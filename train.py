# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17gjczPs_RSwXX74X9fsiefVEu2Kg0Eya

#Use the sweep functionality provided by wandb to find the best values for the hyperparameters.

    
*   number of epochs: 10, 20, 30
*   number of hidden layers: 3, 4, 5
*   size of every hidden layer: 32, 64, 128
*   weight decay (L2 regularisation): 0, 0.0005, 0.5
*   learning rate: 1e-3, 1 e-4
*   optimizer: sgd, momentum, adaDelta, rmsprop, adam, nadam
*   batch size: 16, 32, 64
*   weight initialisation: random, Xavier
*   activation functions: sigmoid, tanh, ReLU

Argparse
"""

#parser.add_argument('-f')

import argparse

parser = argparse.ArgumentParser(description="Stores all the hyperpamaters for the model.")
parser.add_argument("-wp", "--wandb_project",type=str, help="Enter the Name of your Wandb Project")
parser.add_argument("-we", "--wandb_entity",type=str, help="Wandb Entity used to track experiments in the Weights & Biases dashboard.")
parser.add_argument("-ws", "--wandb_sweep", default="False", type=bool, help="If you want to run wandb sweep then give True.")
parser.add_argument("-d", "--dataset", default="fashion_mnist",type=str,choices=["mnist","fashion_mnist"])
parser.add_argument("-e", "--epochs",default="1", type=int, help="Number of epochs to train neural network.")
parser.add_argument("-b", "--batch_size",default="4", type=int, help="Batch size used to train neural network.")
parser.add_argument("-l", "--loss",default="cross_entropy", type=str,choices=["mse", "cross_entropy"], help="Loss function to compute the loss.")
parser.add_argument("-o", "--optimizer",default="sgd", type=str, choices= ["sgd", "momentum", "nesterov", "rmsprop", "adam", "nadam"])
parser.add_argument("-lr", "--learning_rate",default="0.1", type=float, help="Learning rate used to optimize model parameters")
parser.add_argument("-m", "--momentum",default="0.5", type=float, help="Momentum used by momentum and nag optimizers.")
parser.add_argument("-beta", "--beta",default="0.5", type=float, help="Beta used by rmsprop optimizer")
parser.add_argument("-beta1", "--beta1",default="0.5", type=float, help="Beta1 used by adam and nadam optimizers.")
parser.add_argument("-beta2", "--beta2",default="0.5", type=float, help="Beta2 used by adam and nadam optimizers.")
parser.add_argument("-eps", "--epsilon",default="0.000001", type=float, help="Epsilon used by optimizers.")
parser.add_argument("-w_d", "--weight_decay",default="0.0", type=float, help="Weight decay used by optimizers.")
parser.add_argument("-w_i", "--weight_init",default="random", type=str,choices=["random", "xavier"])
parser.add_argument("-nhl", "--num_layers",default="1", type=int, help="Number of hidden layers used in feedforward neural network.")
parser.add_argument("-sz", "--hidden_size",default="4", type=int, help="Number of hidden neurons in a feedforward layer.")
parser.add_argument("-cl", "--num_classes",default="10", type=int, help="Number of nuerons in the output layer.")
parser.add_argument("-a", "--activation",default="sigmoid", type=str, choices=["identity", "sigmoid", "tanh", "ReLU"])


args = parser.parse_args()

wandb_project = args.wandb_project
wandb_entity = args.wandb_entity
wandb_sweep = args.wandb_sweep
dataset = args.dataset
max_epochs = args.epochs
batch_size = args.batch_size
loss_fu = args.loss
optimizer = args.optimizer
learning_rate = args.learning_rate
momentum = args.momentum
beta = args.beta
beta1 = args.beta1
beta2 = args.beta2
eps = args.epsilon
L2_decay = args.weight_decay
weight_init = args.weight_init
hid_layer = args.num_layers + 1
no_of_neuron = args.hidden_size
acti_fun = args.activation
no_of_classes = args.num_classes
print("wandb_project :", wandb_project , "wandb_entity: ", wandb_entity,"dataset: ",dataset,"epochs: ",max_epochs,"batch_size: ",batch_size,"loss_func", loss_fu,"optimizer", optimizer ,"learning_rate: ", learning_rate,"momentum: ", momentum, "beta: ",beta, "beta1:", beta1,"beta2: ", beta2, "lambd: ", L2_decay,"no_of_hidden_layers:", hid_layer, "activation_function: ", acti_fun,"no_of_classes: ", no_of_classes)

"""Importing inbuild functions"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import math

"""Importing user defined functions(developed in the assignment)"""

from activation_fun import sigmoid, tanh, ReLu, softmax, der_sigmoid, der_tanh, der_ReLu, der_softmax
from loss_accuracy import model_loss, model_accuracy
from question_2 import forward_propagation, batch_normalize
from weights_bias import weights_bias
from question_3 import gradient_descent, momentum_gd, adaDelta, nag, rmsprop, adam, nadam

#pip install wandb
import wandb

"""Loading dataset and splitting in train and test data"""

if dataset == "fashion_mnist":
  from keras.datasets import fashion_mnist
  (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()
if dataset == "mnist":
  from keras.datasets import mnist
  (x_train, y_train), (x_test, y_test) = mnist.load_data()

# Normalize the dataset
X_train = x_train.reshape(x_train.shape[0], x_train.shape[1]*x_train.shape[2] )/255
X_test = x_test.reshape(x_test.shape[0], x_test.shape[1]*x_test.shape[2])/255

"""Keeping aside 10% from the train data for validation of our model"""

validation_size = int(len(X_train)*0.1)

# randomly shuffle the indices of the data
shuffled_indices = np.random.permutation(len(X_train))

# split the shuffled data into training and validation sets
train_indices, validation_indices = shuffled_indices[:-validation_size], shuffled_indices[-validation_size:]
X_train, X_validation = X_train[train_indices], X_train[validation_indices]
y_train, y_validation = y_train[train_indices], y_train[validation_indices]

"""function to train the neural network using the optimizers developed in question-3"""

def train_NN(wandb_entity, wandb_project, wandb_sweep, max_epochs, dataset,
             batch_size, no_of_classes, hid_layer, no_of_neuron, loss_fu,
             acti_fun, learning_rate, L2_decay, weight_init, optimizer, 
             momentum, eps, beta, beta1, beta2):
             
  if wandb_sweep == False:
    wandb.init(entity=wandb_entity, project=wandb_project)

  elif wandb_sweep == True:
    # default values- correspond to the hyperparameters of best model trained.
    config_defaults = {
          'max_epochs': 30,
          'batch_size': 8,
          'learning_rate': 1e-3,
          'acti_fun': 'sigmoid',
          'optimizer': 'sgd',
          'weight_init': 'xavier',
          'L2_decay': 0.0005,
          'no_of_neuron': 128,
          'hid_layer': 3,
          'loss_fu':'cross_entropy'
      }
  
    # initialize wandb
    wandb.init(config=config_defaults)

    # config is a data structure that holds hyperparameters and inputs
  
    config = wandb.config

    # Local variables, values obtained from wandb config
    no_of_neuron = config.no_of_neuron
    hid_layer = config.hid_layer
    weight_init = config.weight_init
    max_epochs = config.max_epochs
    batch_size = config.batch_size
    learning_rate = config.learning_rate
    acti_fun = config.acti_fun
    L2_decay = config.L2_decay
    optimizer = config.optimizer
    loss_fu = config.loss_fu

    wandb.run.name  = "e_{}_nhl_{}_shl_{}_lr_{}_opt_{}_bs_{}_W_{}_af_{}_loss_{}".format(max_epochs,
                                                                                hid_layer,
                                                                                no_of_neuron,
                                                                                learning_rate,
                                                                                optimizer,
                                                                                batch_size,
                                                                                weight_init,
                                                                                acti_fun,
                                                                                loss_fu)
                                                                                    
    
    print(wandb.run.name )


  #no_of_classes = len(np.unique(y_train))
  Weights, bias = weights_bias(weight_init, no_of_neuron, hid_layer, X_train, no_of_classes)
  # eps = 1e-10
  # beta = 0.5
  # beta1 = 0.9
  # beta2 = 0.999
  L, n = hid_layer, no_of_neuron
  
  prev_uw, prev_ub = weights_bias(3, n, L, X_train, no_of_classes)

  prev_vw, prev_vb = weights_bias(3, n, L, X_train, no_of_classes)
  
  m_w, m_b = weights_bias(3, n, L, X_train, no_of_classes)

  v_w, v_b = weights_bias(3, n, L, X_train, no_of_classes)

  for epoch in range(max_epochs):

    if optimizer == 'sgd':
      Weights, bias, loss = gradient_descent(learning_rate, Weights, bias, hid_layer, no_of_neuron,
                     y_train, X_train, batch_size, L2_decay, loss_fu, acti_fun, weight_init)
    elif optimizer == 'momentum':
      Weights, bias, loss = momentum_gd(prev_uw, prev_ub, Weights, bias, hid_layer,
                                        no_of_neuron, X_train, y_train,learning_rate,
                                        batch_size, L2_decay, loss_fu, acti_fun,weight_init)
    elif optimizer == 'nesterov':
      Weights, bias, loss = nag(prev_vw, prev_vb, Weights, bias, hid_layer,
                                no_of_neuron, X_train, y_train,learning_rate,
                                batch_size, L2_decay, loss_fu, acti_fun, weight_init)
    elif optimizer == 'rmsprop':
      Weights, bias, loss = rmsprop(v_w, v_b, Weights, bias, hid_layer, no_of_neuron,
                                    X_train, y_train, learning_rate, batch_size,
                                    L2_decay,loss_fu, acti_fun, weight_init)
    elif optimizer == 'adam':
      Weights, bias, loss = adam(eps, beta1, beta2, m_w, m_b, v_w, v_b, Weights,
                                  bias, hid_layer, no_of_neuron, X_train, y_train, 
                                  batch_size, learning_rate, L2_decay, loss_fu, acti_fun, weight_init)
    elif optimizer == 'nadam':   
      Weights, bias, loss = nadam(eps, beta1, beta2, m_w, m_b, v_w, v_b, Weights, bias,
                                  hid_layer, no_of_neuron, X_train, y_train, batch_size,
                                  learning_rate,L2_decay, loss_fu, acti_fun, weight_init)

  
    val_accuracy  = model_accuracy(X_validation, y_validation, Weights, bias, hid_layer, acti_fun, weight_init)
    train_accuracy = model_accuracy(X_test, y_test, Weights, bias, hid_layer, acti_fun, weight_init)
    val_loss      = model_loss(X_validation, y_validation, Weights, bias, hid_layer, loss_fu, L2_decay, X_train, acti_fun, weight_init)
    train_loss     = model_loss(X_train, y_train, Weights, bias, hid_layer, loss_fu, L2_decay, X_train, acti_fun, weight_init)
    
    print("validation accuracy", val_accuracy, "train accuracy", train_accuracy, "validation loss", val_loss, "train loss", train_loss, 'epoch', epoch )
    
    wandb.log({"validation accuracy": val_accuracy, "train accuracy": train_accuracy, "validation loss": val_loss, "train loss": train_loss, 'epoch': epoch})

    wandb.run.name 
    wandb.run.save()
    wandb.run.finish()

  return Weights, bias

if wandb_sweep == False:
  Weights, bias = train_NN(wandb_entity, wandb_project, wandb_sweep, max_epochs, dataset,
             batch_size, no_of_classes, hid_layer, no_of_neuron, loss_fu,
             acti_fun, learning_rate, L2_decay, weight_init, optimizer, 
             momentum, eps, beta, beta1, beta2)

"""Using sweep method of wandb we are running the code to find best hyperparameters for our model"""

if wandb_sweep ==True:
  sweep_config = {"name": wandb_project, "method": "grid"}   # may use random as method if computation power is less
  sweep_config["metric"] = {"name": "val_loss", "goal": "minimize"}

  parameters_dict = {
                "max_epochs": {"values": [10, 20, 30]},
                "hid_layer": {"values": [3, 4, 5]},  
                "no_of_neuron": {"values": [32, 64, 128]},           
                "learning_rate": {"values": [1e-3, 1e-4]},
                "optimizer": {"values": ["sgd","momentum","nesterov","rmsprop","adam","nadam"]},
                "batch_size": {"values": [16, 32, 64]}, 
                "weight_init": {"values": ["random", "xavier"]} ,
                "L2_decay": {"values": [0, 0.0005, 0.5]} ,
                "acti_fun": {"values": ["sigmoid", "tanh", "ReLu"]},  
                  }
  sweep_config["parameters"] = parameters_dict

  sweep_id = wandb.sweep(sweep_config, entity=wandb_entity, project=wandb_project)
  wandb.agent(sweep_id, train_NN(wandb_entity, wandb_project, wandb_sweep, max_epochs, dataset,
             batch_size, no_of_classes, hid_layer, no_of_neuron, loss_fu,
             acti_fun, learning_rate, L2_decay, weight_init, optimizer, 
             momentum, eps, beta, beta1, beta2))