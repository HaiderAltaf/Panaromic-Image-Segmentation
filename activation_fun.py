# -*- coding: utf-8 -*-
"""Activation_fun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v9rnvVH9_Z0dBmTkw3IbpeUAYR_6Vbjb

Activation functions
"""

import numpy as np

def sigmoid(a):
  #a = np.clip(a, -1, 1)  # clipping the value od a
  return 1/(1+np.exp(-a))

def tanh(a):
  return (np.exp(a)-np.exp(-a))/(np.exp(a)+np.exp(-a))

def ReLu(a):  # leaky Relu
  return np.maximum(0 ,a)
  
def softmax(a):
  a = a - np.max(a)
  return np.exp(a)/np.sum(np.exp(a))
   

def der_softmax(a):
  return softmax(a)*(1- softmax(a))
  
def der_sigmoid(a):
  return sigmoid(a)*(1-sigmoid(a))

def der_tanh(a):
  return 1-(tanh(a)*tanh(a))

def der_ReLu(a):

  # it will create a matrix of same dimension as of a.
  gradient = np.zeros_like(a)  
  # sets the entries of gradient to 1 where the corresponding entries of x>=0
  gradient[a >=0] = 1
  gradient[a < 0] = 0

  return gradient